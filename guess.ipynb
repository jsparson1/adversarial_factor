{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: facenet-pytorch in ./.venv/lib/python3.10/site-packages (2.6.0)\n",
      "Requirement already satisfied: matplotlib in ./.venv/lib/python3.10/site-packages (3.9.2)\n",
      "Requirement already satisfied: torch in ./.venv/lib/python3.10/site-packages (2.2.2)\n",
      "Requirement already satisfied: torchvision in ./.venv/lib/python3.10/site-packages (0.17.2)\n",
      "Requirement already satisfied: numpy<2.0.0,>=1.24.0 in ./.venv/lib/python3.10/site-packages (from facenet-pytorch) (1.26.4)\n",
      "Requirement already satisfied: Pillow<10.3.0,>=10.2.0 in ./.venv/lib/python3.10/site-packages (from facenet-pytorch) (10.2.0)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.0.0 in ./.venv/lib/python3.10/site-packages (from facenet-pytorch) (2.32.3)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.0.0 in ./.venv/lib/python3.10/site-packages (from facenet-pytorch) (4.67.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in ./.venv/lib/python3.10/site-packages (from matplotlib) (1.3.0)\n",
      "Requirement already satisfied: cycler>=0.10 in ./.venv/lib/python3.10/site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in ./.venv/lib/python3.10/site-packages (from matplotlib) (4.54.1)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in ./.venv/lib/python3.10/site-packages (from matplotlib) (1.4.7)\n",
      "Requirement already satisfied: packaging>=20.0 in ./.venv/lib/python3.10/site-packages (from matplotlib) (24.1)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in ./.venv/lib/python3.10/site-packages (from matplotlib) (3.2.0)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in ./.venv/lib/python3.10/site-packages (from matplotlib) (2.9.0.post0)\n",
      "Requirement already satisfied: filelock in ./.venv/lib/python3.10/site-packages (from torch) (3.16.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in ./.venv/lib/python3.10/site-packages (from torch) (4.12.2)\n",
      "Requirement already satisfied: sympy in ./.venv/lib/python3.10/site-packages (from torch) (1.13.3)\n",
      "Requirement already satisfied: networkx in ./.venv/lib/python3.10/site-packages (from torch) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in ./.venv/lib/python3.10/site-packages (from torch) (3.1.4)\n",
      "Requirement already satisfied: fsspec in ./.venv/lib/python3.10/site-packages (from torch) (2024.10.0)\n",
      "Requirement already satisfied: six>=1.5 in ./.venv/lib/python3.10/site-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in ./.venv/lib/python3.10/site-packages (from requests<3.0.0,>=2.0.0->facenet-pytorch) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./.venv/lib/python3.10/site-packages (from requests<3.0.0,>=2.0.0->facenet-pytorch) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./.venv/lib/python3.10/site-packages (from requests<3.0.0,>=2.0.0->facenet-pytorch) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./.venv/lib/python3.10/site-packages (from requests<3.0.0,>=2.0.0->facenet-pytorch) (2024.8.30)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in ./.venv/lib/python3.10/site-packages (from jinja2->torch) (3.0.2)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in ./.venv/lib/python3.10/site-packages (from sympy->torch) (1.3.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install facenet-pytorch matplotlib torch torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jonahstorch/Documents/Deep Learning/adversarial_factor/.venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "#can pair down imports later\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import random\n",
    "from collections import deque\n",
    "from facenet_pytorch import MTCNN, InceptionResnetV1\n",
    "from PIL import Image, ImageDraw\n",
    "import os\n",
    "#import dataset from somewhere unless we using a local file for dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize MTCNN for face detection\n",
    "mtcnn = MTCNN()\n",
    "mtcnn.classify = True\n",
    "\n",
    "# Load an image containing faces\n",
    "img = Image.open('./test.jpg')\n",
    "\n",
    "# Detect faces in the image\n",
    "boxes, face_probability = mtcnn.detect(img)\n",
    "\n",
    "# If faces are detected, 'boxes' will contain the bounding box coordinates\n",
    "if boxes is not None:\n",
    "    for box in boxes:\n",
    "        # Draw bounding boxes on the image\n",
    "        draw = ImageDraw.Draw(img)\n",
    "        draw.rectangle(box.tolist(), outline='red', width=3)\n",
    "# Display or save the image with detected faces\n",
    "img.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mtcnn = MTCNN(image_size=(244,244), margin=0, min_face_size=20)\n",
    "resnet = InceptionResnetV1(pretrained='vggface2').eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=244x244 at 0x109770C70>]\n",
      "[<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=244x244 at 0x16944B3A0>]\n"
     ]
    }
   ],
   "source": [
    "def extract_image(filename):\n",
    "\timage = Image.open(filename)\n",
    "\treturn image\n",
    "\n",
    "def load_face_embeddings(celeb_name):\n",
    "    dir = f'./faces/{celeb_name}'\n",
    "    faces = []\n",
    "    \n",
    "    for filename in os.listdir(dir):\n",
    "        path = f'{dir}/{filename}'\n",
    "        image = Image.open(path)\n",
    "        face = mtcnn(image) \n",
    "        face_embedding = resnet(face.unsqueeze(0))\n",
    "        faces.append(face_embedding)\n",
    "    return faces\n",
    "\n",
    "def load_data():\n",
    "    DATA_DIR='./faces'\n",
    "    celeb_names = [i.name for i in os.scandir(DATA_DIR) if i.is_dir()]\n",
    "    names = []\n",
    "    embeddings = []\n",
    "    for celeb_name in celeb_names:\n",
    "        faces_of_celeb = load_face_embeddings(celeb_name)\n",
    "        names.append(celeb_name)\n",
    "        embeddings.append(faces_of_celeb)\n",
    "    return embeddings,names\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
